{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riemannian Optimisation with Pymanopt for Inference in MoG models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mixture of Gaussians (MoG) model assumes that datapoints $\\mathbf{x}_i\\in\\mathbb{R}^d$ follow a distribution described by the following probability density function:\n",
    "\n",
    "$p(\\mathbf{x}) = \\sum_{m=1}^M \\pi_m p_\\mathcal{N}(\\mathbf{x};\\mathbf{\\mu}_m,\\mathbf{\\Sigma}_m)$ where $\\pi_m$ is the probability that the data point belongs to the $m^\\text{th}$ mixture component and $p_\\mathcal{N}(\\mathbf{x};\\mathbf{\\mu}_m,\\mathbf{\\Sigma}_m)$ is the probability density function of a multivariate Gaussian distribution with mean $\\mathbf{\\mu}_m \\in \\mathbb{R}^d$ and psd covariance matrix $\\mathbf{\\Sigma}_m \\in \\{\\mathbf{M}\\in\\mathbb{R}^{d\\times d}: \\mathbf{M}\\succeq 0\\}$.\n",
    "\n",
    "As an example consider the mixture of three Gaussians with means\n",
    "$\\mathbf{\\mu}_1 = \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}$,\n",
    "$\\mathbf{\\mu}_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and\n",
    "$\\mathbf{\\mu}_3 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$, covariances\n",
    "$\\mathbf{\\Sigma}_1 = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n",
    "$\\mathbf{\\Sigma}_2 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 3 \\end{bmatrix}$ and\n",
    "$\\mathbf{\\Sigma}_3 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$\n",
    "and mixture probability vector $\\pi=\\left[0.1, 0.6, 0.3\\right]$.\n",
    "Let's generate $N=1000$ samples of that MoG model and scatter plot the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of data\n",
    "N = 1000\n",
    "\n",
    "# Dimension of data\n",
    "D = 2\n",
    "\n",
    "# Number of clusters\n",
    "K = 3\n",
    "\n",
    "pi = [0.1, 0.6, 0.3]\n",
    "mu = [np.array([-4, 1]), np.array([0, 0]), np.array([2, -1])]\n",
    "Sigma = [np.array([[3, 0],[0, 1]]), np.array([[1, 1.], [1, 3]]), .5 * np.eye(2)]\n",
    "\n",
    "components = np.random.choice(K, size=N, p=pi)\n",
    "samples = np.zeros((N, D))\n",
    "# for each component, generate all needed samples\n",
    "for k in range(K):\n",
    "    # indices of current component in X\n",
    "    indices = (k == components)\n",
    "    # number of those occurrences\n",
    "    n_k = indices.sum()\n",
    "    if n_k > 0:\n",
    "        samples[indices] = np.random.multivariate_normal(mu[k], Sigma[k], n_k)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'c', 'm']\n",
    "for i in range(K):\n",
    "    indices = (i == components)\n",
    "    plt.scatter(samples[indices, 0], samples[indices, 1], alpha=.4, color=colors[i%K])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data sample the de facto standard method to infer the parameters is the [expectation maximisation](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) (EM) algorithm that, in alternating so-called E and M steps, maximises the log-likelihood of the data.\n",
    "In [arXiv:1506.07677](http://arxiv.org/pdf/1506.07677v1.pdf) Hosseini and Sra propose Riemannian optimisation as a powerful counterpart to EM. Importantly, they introduce a reparameterisation that leaves local optima of the log-likelihood unchanged while resulting in a geodesically convex optimisation problem over a product manifold $\\prod_{m=1}^M\\mathcal{PD}^{(d+1)\\times(d+1)}$ of manifolds of $(d+1)\\times(d+1)$ positive definite matrices.\n",
    "The proposed method is on par with EM and shows less variability in running times.\n",
    "\n",
    "The reparameterised optimisation problem for augmented data points $\\mathbf{y}_i=[\\mathbf{x}_i\\ 1]$ can be stated as follows:\n",
    "\n",
    "$$\\min_{(S_1, ..., S_m, \\nu_1, ..., \\nu_{m-1}) \\in \\prod_{m=1}^M \\mathcal{PD}^{(d+1)\\times(d+1)}\\times\\mathbb{R}^{M-1}}\n",
    "-\\sum_{n=1}^N\\log\\left(\n",
    "\\sum_{m=1}^M \\frac{\\exp(\\nu_m)}{\\sum_{k=1}^M\\exp(\\nu_k)}\n",
    "q_\\mathcal{N}(\\mathbf{y}_n;\\mathbf{S}_m)\n",
    "\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\mathcal{PD}^{(d+1)\\times(d+1)}$ is the manifold of positive definite\n",
    "$(d+1)\\times(d+1)$ matrices\n",
    "* $\\mathcal{\\nu}_m = \\log\\left(\\frac{\\alpha_m}{\\alpha_M}\\right), \\ m=1, ..., M-1$ and $\\nu_M=0$\n",
    "* $q_\\mathcal{N}(\\mathbf{y}_n;\\mathbf{S}_m) =\n",
    "2\\pi\\exp\\left(\\frac{1}{2}\\right)\n",
    "|\\operatorname{det}(\\mathbf{S}_m)|^{-\\frac{1}{2}}(2\\pi)^{-\\frac{d+1}{2}}\n",
    "\\exp\\left(-\\frac{1}{2}\\mathbf{y}_i^\\top\\mathbf{S}_m^{-1}\\mathbf{y}_i\\right)$\n",
    "\n",
    "**Optimisation problems like this can easily be solved using Pymanopt – even without the need to differentiate the cost function manually!**\n",
    "\n",
    "So let's infer the parameters of our toy example by Riemannian optimisation using Pymanopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from pymanopt.manifolds import Product, Euclidean, PositiveDefinite\n",
    "from pymanopt import Problem\n",
    "from pymanopt.solvers import SteepestDescent\n",
    "\n",
    "# (1) Instantiate the manifold\n",
    "manifold = Product([PositiveDefinite(D+1, k=K), Euclidean(K-1)])\n",
    "\n",
    "# (2) Define cost function\n",
    "# The parameters must be contained in a list theta.\n",
    "def cost(theta):\n",
    "    # Unpack parameters\n",
    "    nu = np.hstack([theta[1], [0]])\n",
    "    \n",
    "    S = theta[0]\n",
    "    logdetS = np.expand_dims(np.linalg.slogdet(S)[1], 1)\n",
    "    y = np.vstack([samples.T, np.ones((1, N))])\n",
    "\n",
    "    # Calculate log_q\n",
    "    y = np.expand_dims(y, 0)\n",
    "    \n",
    "    # 'Probability' of y belonging to each cluster\n",
    "    log_q = -0.5 * (np.sum(y * np.linalg.solve(S, y), axis=1) + logdetS)\n",
    "\n",
    "    alpha = np.exp(nu)\n",
    "    alpha = alpha / np.sum(alpha)\n",
    "    alpha = np.expand_dims(alpha, 1)\n",
    "    \n",
    "    loglikvec = logsumexp(np.log(alpha) + log_q, axis=0)\n",
    "    return -np.sum(loglikvec)\n",
    "\n",
    "problem = Problem(manifold=manifold, cost=cost, verbosity=1)\n",
    "\n",
    "# (3) Instantiate a Pymanopt solver\n",
    "solver = SteepestDescent()\n",
    "\n",
    "# let Pymanopt do the rest\n",
    "Xopt = solver.solve(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Pymanopt has finished the optimisation we can obtain the inferred parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu1hat = Xopt[0][0][0:2,2:3]\n",
    "Sigma1hat = Xopt[0][0][:2, :2] - mu1hat.dot(mu1hat.T)\n",
    "mu2hat = Xopt[0][1][0:2,2:3]\n",
    "Sigma2hat = Xopt[0][1][:2, :2] - mu2hat.dot(mu2hat.T)\n",
    "mu3hat = Xopt[0][2][0:2,2:3]\n",
    "Sigma3hat = Xopt[0][2][:2, :2] - mu3hat.dot(mu3hat.T)\n",
    "pihat = (np.exp(np.hstack([Xopt[1], [0]])) / \n",
    "      np.sum(np.exp(np.hstack([Xopt[1], [0]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convince ourselves that the inferred parameters are close to the ground truth parameters.\n",
    "\n",
    "The ground truth parameters $\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1, \\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2, \\mathbf{\\mu}_3, \\mathbf{\\Sigma}_3, \\pi_1, \\pi_2, \\pi_3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mu[0])\n",
    "print(Sigma[0])\n",
    "print(mu[1])\n",
    "print(Sigma[1])\n",
    "print(mu[2])\n",
    "print(Sigma[2])\n",
    "print(pi[0])\n",
    "print(pi[1])\n",
    "print(pi[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the inferred parameters $\\hat{\\mathbf{\\mu}}_1, \\hat{\\mathbf{\\Sigma}}_1, \\hat{\\mathbf{\\mu}}_2, \\hat{\\mathbf{\\Sigma}}_2, \\hat{\\mathbf{\\mu}}_3, \\hat{\\mathbf{\\Sigma}}_3, \\hat{\\pi}_1, \\hat{\\pi}_2, \\hat{\\pi}_3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mu1hat)\n",
    "print(Sigma1hat)\n",
    "print(mu2hat)\n",
    "print(Sigma2hat)\n",
    "print(mu3hat)\n",
    "print(Sigma3hat)\n",
    "print(pihat[0])\n",
    "print(pihat[1])\n",
    "print(pihat[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà – this was a brief demonstration of how to do inference for MoG models by performing Manifold optimisation using Pymanopt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
